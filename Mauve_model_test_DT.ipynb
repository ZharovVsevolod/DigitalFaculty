{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ранагон\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils.poetry import *\n",
    "from utils.base import *\n",
    "from utils.transformer_tools import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import youtokentome as yttm\n",
    "import mauve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "horus = load_chunks(\"./datasets/DarkTower/DT14_red.txt\", chunk_size=300)\n",
    "\n",
    "ALL_LEN = len(horus)\n",
    "len_for_steps = int(ALL_LEN / 3)\n",
    "horus_for_GAN = horus[len_for_steps * 2:]\n",
    "\n",
    "for_mauve_test = horus_for_GAN[len(horus_for_GAN)-100:]\n",
    "\n",
    "print(len(for_mauve_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE_MODEL_FILENAME = './models/tokens/dt_bpe_2000.yttm'\n",
    "tokenizer = yttm.BPE(BPE_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_model = Language_Model(\n",
    "    vocab_size = tokenizer.vocab_size(),\n",
    "    embedding_size = 512,\n",
    "    backbone = Transformer_Encoder(\n",
    "        nn.TransformerEncoderLayer(\n",
    "            d_model = 512,\n",
    "            nhead = 16,\n",
    "            dim_feedforward = 1024,\n",
    "            dropout = 0.3\n",
    "        ),\n",
    "        num_layers=5\n",
    "    ),\n",
    "    emb_dropout = 0.2\n",
    ")\n",
    "\n",
    "G_model_gan = Language_Model(\n",
    "    vocab_size = tokenizer.vocab_size(),\n",
    "    embedding_size = 512,\n",
    "    backbone = Transformer_Encoder(\n",
    "        nn.TransformerEncoderLayer(\n",
    "            d_model = 512,\n",
    "            nhead = 16,\n",
    "            dim_feedforward = 1024,\n",
    "            dropout = 0.3\n",
    "        ),\n",
    "        num_layers=5\n",
    "    ),\n",
    "    emb_dropout = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_model_small = Language_Model(\n",
    "    vocab_size = tokenizer.vocab_size(),\n",
    "    embedding_size = 512,\n",
    "    backbone = Transformer_Encoder(\n",
    "        nn.TransformerEncoderLayer(\n",
    "            d_model = 512,\n",
    "            nhead = 16,\n",
    "            dim_feedforward = 1024,\n",
    "            dropout = 0.3\n",
    "        ),\n",
    "        num_layers=5\n",
    "    ),\n",
    "    emb_dropout = 0.2\n",
    ")\n",
    "\n",
    "G_model_small_gan = Language_Model(\n",
    "    vocab_size = tokenizer.vocab_size(),\n",
    "    embedding_size = 512,\n",
    "    backbone = Transformer_Encoder(\n",
    "        nn.TransformerEncoderLayer(\n",
    "            d_model = 512,\n",
    "            nhead = 16,\n",
    "            dim_feedforward = 1024,\n",
    "            dropout = 0.3\n",
    "        ),\n",
    "        num_layers=5\n",
    "    ),\n",
    "    emb_dropout = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_model_gan.load_state_dict(torch.load('./models/GAN/DT/1/model_G.pth'))\n",
    "G_model_gan.cuda()\n",
    "\n",
    "G_model.load_state_dict(torch.load('./models/Lord/DT/1/Lord.pth'))\n",
    "G_model.cuda()\n",
    "\n",
    "G_model_small_gan.load_state_dict(torch.load('./models/GAN/DT/2/model_G.pth'))\n",
    "G_model_gan.cuda()\n",
    "\n",
    "G_model_small.load_state_dict(torch.load('./models/Lord/DT/2/Lord.pth'))\n",
    "G_model.cuda()\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_generator = Gen_method(G_model, tokenizer)\n",
    "fake_texts = []\n",
    "\n",
    "loop_generator_gan = Gen_method(G_model_gan, tokenizer)\n",
    "fake_texts_gan = []\n",
    "\n",
    "loop_generator_small = Gen_method(G_model_small, tokenizer)\n",
    "fake_texts_small = []\n",
    "\n",
    "loop_generator_small_gan = Gen_method(G_model_small_gan, tokenizer)\n",
    "fake_texts_small_gan = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]d:\\Programs\\DigitalFaculty\\utils\\poetry.py:205: RuntimeWarning: invalid value encountered in log\n",
      "  distribution = np.log(original) / temperature\n",
      "d:\\Programs\\DigitalFaculty\\utils\\poetry.py:267: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  next_tokens_logproba = F.log_softmax(next_tokens_logits)\n",
      "100%|██████████| 100/100 [07:15<00:00,  4.35s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(for_mauve_test))):\n",
    "    fake_texts.append(loop_generator(for_mauve_test[i], N_cut=15, need_reweight=True, temper=0.3, alpha=0.15)[0])\n",
    "    fake_texts_gan.append(loop_generator_gan(for_mauve_test[i], N_cut=15, need_reweight=True, temper=0.3, alpha=0.15)[0])\n",
    "    fake_texts_small.append(loop_generator_small(for_mauve_test[i], N_cut=15, need_reweight=True, temper=0.3, alpha=0.15)[0])\n",
    "    fake_texts_small_gan.append(loop_generator_small_gan(for_mauve_test[i], N_cut=15, need_reweight=True, temper=0.3, alpha=0.15)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to know you as it is.” “I don’t know, but that’s a name what it really?” Susannah asked. “Yes.” “Then we’ll be drumoned. “Is it possible, if you want to use\n",
      "to know you as soon begin,” said Mr. Bissette’s name complit. “I A Boy YORK.” The word came out with my mindfirection for a couple of\n",
      "to know you as your Final Esay, and Engineer Bob and Engineer Bob heard Engineer Bob inscome, Charlie was Charlie’s. Charlie the Ch\n",
      "to know you as me?” “I don’t know.” “I don’t know,” Roland said, “but that was apt to dollar it.” “If we’re going to kill use the Beam.” “I’m sure\n"
     ]
    }
   ],
   "source": [
    "print(fake_texts[4])\n",
    "print(fake_texts_gan[4])\n",
    "print(fake_texts_small[4])\n",
    "print(fake_texts_small_gan[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_texts_to_file(fake_texts_gan, \"./datasets/DarkTower/DT_fake_rankgen.txt\")\n",
    "save_texts_to_file(fake_texts, \"./datasets/DarkTower/DT_fake_rankgen_old.txt\")\n",
    "save_texts_to_file(fake_texts_small_gan, \"./datasets/DarkTower/DTS_fake_rankgen.txt\")\n",
    "save_texts_to_file(fake_texts_small, \"./datasets/DarkTower/DTS_fake_rankgen_old.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing p: 100%|██████████| 100/100 [01:58<00:00,  1.19s/it]\n",
      "Featurizing q: 100%|██████████| 100/100 [01:01<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46440026495076625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing p: 100%|██████████| 100/100 [01:58<00:00,  1.19s/it]\n",
      "Featurizing q: 100%|██████████| 100/100 [01:02<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38434915683708215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing p: 100%|██████████| 100/100 [01:58<00:00,  1.19s/it]\n",
      "Featurizing q: 100%|██████████| 100/100 [01:03<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1635476586189541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing p: 100%|██████████| 100/100 [01:58<00:00,  1.19s/it]\n",
      "Featurizing q: 100%|██████████| 100/100 [01:04<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.311050106387855\n"
     ]
    }
   ],
   "source": [
    "out_old = mauve.compute_mauve(p_text=for_mauve_test, q_text=fake_texts)\n",
    "print(out_old.mauve)\n",
    "\n",
    "out_gan = mauve.compute_mauve(p_text=for_mauve_test, q_text=fake_texts_gan)\n",
    "print(out_gan.mauve)\n",
    "\n",
    "out_old_small = mauve.compute_mauve(p_text=for_mauve_test, q_text=fake_texts_small)\n",
    "print(out_old_small.mauve)\n",
    "\n",
    "out_gan_small = mauve.compute_mauve(p_text=for_mauve_test, q_text=fake_texts_small_gan)\n",
    "print(out_gan_small.mauve)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
